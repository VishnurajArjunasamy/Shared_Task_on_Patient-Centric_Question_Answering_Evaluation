{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5684e78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "from evaluate import load\n",
    "import regex as reg\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fdcd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_DIR = \"\"  # Path for your prediction dir\n",
    "GT_DIR = \"../NLP4Health_Shared_task_benchmark/test_data_internal_with_answers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEAM_NAME = os.path.basename(PRED_DIR).split('_')[0]\n",
    "TASK_NAME = os.path.basename(PRED_DIR).split('_')[1]\n",
    "TASK_TYPE = os.path.basename(PRED_DIR).split('_')[2]\n",
    "\n",
    "RESULTS_DIR = \"../Evaluation_Results/Submissions\"\n",
    "OUT_FILE = f\"{TEAM_NAME}_{TASK_NAME}_{TASK_TYPE}_v1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8345ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059672cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_metric = load('comet',config_name=\"Unbabel/wmt22-comet-da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bert_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41021db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_code(language):\n",
    "    \"\"\"\n",
    "    Returns the language code compatible with BERTScore or IndicBERT.\n",
    "    Fallbacks to 'multilingual' if unsupported.\n",
    "    \"\"\"\n",
    "    lang_map = {\n",
    "        \"Assamese\": \"as\",\n",
    "        \"Telugu\": \"te\",\n",
    "        \"Tamil\": \"ta\",\n",
    "        \"Kannada\": \"kn\",\n",
    "        \"Hindi\": \"hi\",\n",
    "        \"Marathi\": \"mr\",\n",
    "        \"Bangla\": \"bn\",\n",
    "        \"Dogri\": \"doi\",\n",
    "        \"Gujarati\": \"gu\",\n",
    "        \"English\":'en'\n",
    "    }\n",
    "    return lang_map.get(language, \"multilingual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e471ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def read_text(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "def flatten_kv(d):\n",
    "    return \" \".join(f\"{k}: {d[k]}\" for k in sorted(d.keys()))\n",
    "\n",
    "def normalize(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    text = reg.sub(r'\\s+', ' ', text)\n",
    "    text = reg.sub(r'\\p{P}+', '', text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    if not text or text in [None, \"\", []]:\n",
    "        return Counter()\n",
    "    return Counter(str(text))\n",
    "\n",
    "def read_jsonl(path):\n",
    "    dialogues = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue \n",
    "            try:\n",
    "                dialogues.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid line in {path}: {line}\")\n",
    "    return dialogues        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_template(data, template):    \n",
    "    if isinstance(template, dict):\n",
    "        result = {}\n",
    "        \n",
    "        # Ensure data is a dict, otherwise treat as empty\n",
    "        if not isinstance(data, dict):\n",
    "            data = {}\n",
    "        \n",
    "        # Process each key in template\n",
    "        for key, subtemplate in template.items():\n",
    "            if key in data:\n",
    "                # If template value is primitive/None but data is dict, flatten it\n",
    "                if (isinstance(subtemplate, (str, int, float, bool, type(None))) and \n",
    "                    isinstance(data[key], dict)):\n",
    "                    result[key] = flatten_dict_values(data[key])\n",
    "                else:\n",
    "                    result[key] = fill_with_template(data[key], subtemplate)\n",
    "            else:\n",
    "                # Key missing → fill with template structure\n",
    "                result[key] = fill_with_template(None, subtemplate)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Handle list templates\n",
    "    elif isinstance(template, list):\n",
    "        # Ensure data is a list, otherwise treat as empty\n",
    "        if not isinstance(data, list):\n",
    "            data = []\n",
    "        \n",
    "        if not template:\n",
    "            # Empty template = empty list\n",
    "            return []\n",
    "        \n",
    "        filled = []\n",
    "        template_len = len(template)\n",
    "        \n",
    "        # Always return exactly template_len items\n",
    "        for i in range(template_len):\n",
    "            template_item = template[i]\n",
    "            \n",
    "            # Get data item if available, otherwise None\n",
    "            if i < len(data):\n",
    "                item_data = data[i]\n",
    "            else:\n",
    "                item_data = None\n",
    "            \n",
    "            filled.append(fill_with_template(item_data, template_item))\n",
    "        \n",
    "        return filled\n",
    "    \n",
    "    # Handle primitives (str, int, bool, None, etc.)\n",
    "    else:\n",
    "        # If template is None, flatten data if it's multi-dimensional or dict\n",
    "        if template is None:\n",
    "            if data is None:\n",
    "                return \"Missing\"\n",
    "            if isinstance(data, dict):\n",
    "                return flatten_dict_values(data)\n",
    "            return flatten_value_v2(data)\n",
    "        \n",
    "        # If data is None or not provided, return \"Missing\"\n",
    "        # Otherwise, keep the existing value\n",
    "        return data if data is not None else \"Missing\"\n",
    "\n",
    "\n",
    "def flatten_value_v2(value):\n",
    "\n",
    "    if isinstance(value, list):\n",
    "        result = []\n",
    "        for item in value:\n",
    "            if isinstance(item, list):\n",
    "                # Recursively flatten nested lists\n",
    "                result.extend(flatten_value_v2(item))\n",
    "            elif isinstance(item, dict):\n",
    "                # Extract all values from dict\n",
    "                result.extend(flatten_dict_values(item))\n",
    "            else:\n",
    "                result.append(item)\n",
    "        return result\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "def flatten_dict_values(d):\n",
    "    result = []\n",
    "    for value in d.values():\n",
    "        if isinstance(value, dict):\n",
    "            result.extend(flatten_dict_values(value))\n",
    "        elif isinstance(value, list):\n",
    "            result.extend(flatten_value_v2(value))\n",
    "        else:\n",
    "            result.append(value)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten_knv(List[Dict])\n",
    "def flatten_knv(knv_list):\n",
    "    values = []\n",
    "\n",
    "    if isinstance(knv_list, dict):\n",
    "        for v in knv_list.values():\n",
    "            values.extend(flatten_knv(v))\n",
    "\n",
    "    elif isinstance(knv_list, list):\n",
    "        if len(knv_list) == 0:\n",
    "            values.append(\"\")  # empty list → empty string for scoring\n",
    "        else:\n",
    "            # If it's a list of simple types (not dicts/lists), join as string\n",
    "            if all(not isinstance(i, (dict, list)) for i in knv_list):\n",
    "                joined = \" \".join(str(i) for i in knv_list if i not in [None, \"\"])\n",
    "                values.append(joined)\n",
    "            else:\n",
    "                # Nested structure — recurse\n",
    "                for item in knv_list:\n",
    "                    values.extend(flatten_knv(item))\n",
    "    else:\n",
    "        values.append(knv_list)\n",
    "\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763873e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(preds, refs):\n",
    "    total_tp, total_fp, total_fn = 0, 0, 0\n",
    "    assert len(preds) == len(refs), \"Preds and refs must be same length.\"\n",
    "\n",
    "    pred_tokens = []\n",
    "    ref_tokens = []\n",
    "\n",
    "    for pred, ref in zip(preds, refs):\n",
    "\n",
    "        pred_tokens = tokenize(normalize(pred))\n",
    "        ref_tokens = tokenize(normalize(ref))\n",
    "\n",
    "        common = sum((pred_tokens & ref_tokens).values())\n",
    "\n",
    "        tp = common\n",
    "        fp = sum(pred_tokens.values()) - common\n",
    "        fn = sum(ref_tokens.values()) - common\n",
    "\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) else 0.0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "    return round(f1, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe889a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_knv_f1_score(preds_knv_list, refs_knv_list):\n",
    "\n",
    "    assert len(preds_knv_list) == len(refs_knv_list), \"Preds knv and refs knv must be same length.\"\n",
    "    if all(p in[\"\", None] or p == {}  for p in preds_knv_list):\n",
    "        return 0.0\n",
    "\n",
    "    all_f1s = []\n",
    "\n",
    "    for pred_knv, ref_knv in zip(preds_knv_list, refs_knv_list):\n",
    "        pred_filled = fill_with_template(pred_knv, ref_knv)\n",
    "        pred_values = flatten_knv(pred_filled)\n",
    "        ref_values  = flatten_knv(ref_knv)\n",
    "\n",
    "        if len(pred_values) != len(ref_values):\n",
    "            print(pred_filled)\n",
    "            print(ref_knv)\n",
    "            print(pred_values)\n",
    "            print(ref_values)\n",
    "\n",
    "        assert len(pred_values) == len(ref_values), \"Preds and refs must be same length.\"\n",
    "\n",
    "        pred_values = [normalize(str(p)) if p is not None else \"None\" for p in pred_values]\n",
    "        ref_values = [normalize(str(r)) if r is not None else \"None\" for r in ref_values]\n",
    "\n",
    "        for pred_val, ref_val in zip(pred_values, ref_values):\n",
    "\n",
    "            # Handle both empty or None\n",
    "            if pred_val in [None, \"\", []] and ref_val in [None, \"\", []]:\n",
    "                f1 = 1.0\n",
    "            else:\n",
    "                pred_tokens = tokenize(pred_val)\n",
    "                ref_tokens  = tokenize(ref_val)\n",
    "\n",
    "                common = sum((pred_tokens & ref_tokens).values())\n",
    "                tp = common\n",
    "                fp = sum(pred_tokens.values()) - common\n",
    "                fn = sum(ref_tokens.values()) - common\n",
    "\n",
    "                precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "                recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "            all_f1s.append(f1)\n",
    "\n",
    "    return round(sum(all_f1s) / len(all_f1s), 4) if all_f1s else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e05dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match score \n",
    "def compute_exact_match(preds, refs):\n",
    "\n",
    "    assert len(preds) == len(refs), \"Preds and refs must be same length.\"\n",
    "\n",
    "    total_matches = 0\n",
    "\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        if normalize(pred) == normalize(ref):\n",
    "            total_matches += 1\n",
    "\n",
    "    exact_match_score = total_matches / len(refs) if len(refs) else 0.0\n",
    "    return round(exact_match_score, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb7f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_knv_exact_match(preds_lists,refs_lists):\n",
    "    assert len(preds_lists) == len(refs_lists), \"Preds knv and refs knv must be same length.\"\n",
    "    if all(p in[\"\", None] or p == {}  for p in preds_lists):\n",
    "        return 0.0\n",
    "    \n",
    "    total_matches = 0\n",
    "    total_pairs = 0\n",
    "    for pred_knv, ref_knv in zip(preds_lists, refs_lists):\n",
    "        pred_filled = fill_with_template(pred_knv, ref_knv)\n",
    "        pred_values = flatten_knv(pred_filled)\n",
    "        ref_values  = flatten_knv(ref_knv)\n",
    "        \n",
    "        # max_len = max(len(pred_values), len(ref_values))\n",
    "        # pred_values += [\"\"] * (max_len - len(pred_values))\n",
    "        # ref_values  += [\"\"] * (max_len - len(ref_values))\n",
    "\n",
    "        # print(f\"Pred {pred_values}\")\n",
    "        # print(f\"Pred {pred_filled}\")\n",
    "        # print(f\"Ref {ref_values}\")\n",
    "        # print(f\"Ref {ref_knv}\")\n",
    "\n",
    "        pred_values = [normalize(str(p)) if p is not None else \"None\" for p in pred_values]\n",
    "        ref_values = [normalize(str(r)) if r is not None else \"None\" for r in ref_values]\n",
    "\n",
    "        assert len(pred_values) == len(ref_values), \"Preds and refs must be same length.\"\n",
    "        for pred_val, ref_val in zip(pred_values, ref_values):\n",
    "            if pred_val == ref_val:\n",
    "                total_matches += 1\n",
    "                \n",
    "        total_pairs += len(ref_values)\n",
    "\n",
    "    exact_match_score = total_matches/ total_pairs if total_pairs else 0.0\n",
    "    return round(exact_match_score,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(preds,refs,lang='en'):\n",
    "\n",
    "    print(lang)\n",
    "\n",
    "    assert len(preds) == len(refs), \"preds and refs should be of same size\"\n",
    "    \n",
    "    # Handle empty refs\n",
    "    if not preds or not refs or all(not p for p in preds):\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    indic_langs = {\"as\",\"bn\",\"gu\",\"hi\",\"kn\",\"ml\",\"mr\",\"or\",\"pa\",\"ta\",\"te\",\"doi\"}\n",
    "\n",
    "    bert_lang = None\n",
    "    model_type = None\n",
    "\n",
    "    if lang in indic_langs:\n",
    "        model_type = \"xlm-roberta-large\"\n",
    "        bert_lang = 'xx'\n",
    "    if lang == \"en\":\n",
    "        model_type = \"roberta-large\"  \n",
    "        bert_lang = lang\n",
    "\n",
    "    # Flatten list of lists\n",
    "    pred_texts = [normalize(p) for p in preds]\n",
    "    ref_texts  = [normalize(r) for r in refs]\n",
    "\n",
    "    # Batch compute BERTScore\n",
    "    P, R, F1 = bert_score(\n",
    "        cands=pred_texts,\n",
    "        refs=ref_texts,\n",
    "        lang=bert_lang,\n",
    "        model_type=model_type,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"precision\": float(P.mean()),\n",
    "        \"recall\": float(R.mean()),\n",
    "        \"f1\": float(F1.mean())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e589d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_knv_bertscore(preds_knv_list, refs_knv_list, lang=\"en\"):\n",
    "    assert len(preds_knv_list) == len(refs_knv_list), \"preds and refs must be of same length\"\n",
    "\n",
    "     # Handle empty refs\n",
    "    if all(p in[\"\", None] or p == {}  for p in preds_knv_list):\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    indic_langs = {\"as\",\"bn\",\"gu\",\"hi\",\"kn\",\"ml\",\"mr\",\"or\",\"pa\",\"ta\",\"te\",\"doi\"}\n",
    "\n",
    "    bert_lang = None\n",
    "    model_type = None\n",
    "\n",
    "    if lang in indic_langs:\n",
    "        model_type = \"xlm-roberta-large\"\n",
    "        bert_lang = 'xx'\n",
    "    if lang == \"en\":\n",
    "        model_type = \"roberta-large\"  \n",
    "        bert_lang = lang\n",
    "\n",
    "    all_f1 = []\n",
    "    all_precision = []\n",
    "    all_recall = []\n",
    "    \n",
    "    # loop on summary_knvs\n",
    "    for pred_knv, ref_knv in zip(preds_knv_list, refs_knv_list):\n",
    "\n",
    "        pred_filled = fill_with_template(pred_knv, ref_knv)\n",
    "        pred_values = flatten_knv(pred_filled)\n",
    "        ref_values  = flatten_knv(ref_knv)\n",
    "\n",
    "        # max_len = max(len(pred_values), len(ref_values))\n",
    "        # pred_values += [\"\"] * (max_len - len(pred_values))\n",
    "        # ref_values  += [\"\"] * (max_len - len(ref_values))\n",
    "\n",
    "        assert len(pred_values) == len(ref_values), \"Preds and refs must be same length.\"\n",
    "\n",
    "        if not pred_values:\n",
    "            all_f1.append(0.0)\n",
    "            all_precision.append(0.0)\n",
    "            all_recall.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        pred_values = [normalize(str(p)) if p is not None else \"None\" for p in pred_values]\n",
    "        ref_values = [normalize(str(r)) if r is not None else \"None\" for r in ref_values]\n",
    "\n",
    "        P, R, F1 = bert_score(\n",
    "            refs=ref_values,\n",
    "            cands=pred_values,\n",
    "            lang=bert_lang,\n",
    "            model_type=model_type,\n",
    "            verbose=False,\n",
    "        )\n",
    "        all_f1.append(float(F1.mean()))\n",
    "        all_precision.append(float(P.mean()))\n",
    "        all_recall.append(float(R.mean()))\n",
    "\n",
    "    return {\n",
    "        \"precision\":sum(all_precision)/len(all_precision) if all_precision else 0,\n",
    "        \"recall\":sum(all_recall)/len(all_recall) if all_recall else 0,\n",
    "        \"f1\": sum(all_f1) / len(all_f1) if all_f1 else 0\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ed37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(preds, refs, lang='en'):\n",
    "    use_stemmer = True\n",
    "    indic_langs = {\"as\",\"bn\",\"gu\",\"hi\",\"kn\",\"ml\",\"mr\",\"or\",\"pa\",\"ta\",\"te\",\"doi\"}\n",
    "    if lang in indic_langs:\n",
    "        use_stemmer = False\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer = use_stemmer)\n",
    "\n",
    "    metrics = {\n",
    "        \"rouge1\": {\"precision\": [], \"recall\": [], \"f1\": []},\n",
    "        \"rouge2\": {\"precision\": [], \"recall\": [], \"f1\": []},\n",
    "        \"rougeL\": {\"precision\": [], \"recall\": [], \"f1\": []},\n",
    "    }\n",
    "\n",
    "    assert len(preds) == len(refs), \"Preds and Refs should be same size\"\n",
    "    preds = [normalize(str(p)) if p else \"\" for p in preds]\n",
    "    refs = [normalize(str(r)) if r else \"\" for r in refs]\n",
    "\n",
    "    for p, r in zip(preds, refs):\n",
    "        score = scorer.score(r, p)\n",
    "        for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "            metrics[metric][\"precision\"].append(score[metric].precision)\n",
    "            metrics[metric][\"recall\"].append(score[metric].recall)\n",
    "            metrics[metric][\"f1\"].append(score[metric].fmeasure)\n",
    "\n",
    "    avg_metrics = {\n",
    "        metric: {\n",
    "            \"precision\": sum(values[\"precision\"]) / len(values[\"precision\"]) if values['precision'] else 0,\n",
    "            \"recall\": sum(values[\"recall\"]) / len(values[\"recall\"])if values['recall'] else 0,\n",
    "            \"f1\": sum(values[\"f1\"]) / len(values[\"f1\"]) if values['f1'] else 0,\n",
    "        }\n",
    "        for metric, values in metrics.items()\n",
    "    }\n",
    "\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c1383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_knv_rouge(preds_knv_list, refs_knv_list, lang = \"en\",):\n",
    "    assert len(preds_knv_list) == len(refs_knv_list), \"Preds knv list and Refs knv list should be same size\"\n",
    "\n",
    "    if all(p in[\"\", None] or p == {}  for p in preds_knv_list):\n",
    "        return {\n",
    "            \"rouge1\": {\"precision\": 0, \"recall\": 0, \"f1\": 0},\n",
    "            \"rouge2\": {\"precision\": 0, \"recall\": 0, \"f1\": 0},\n",
    "            \"rougeL\": {\"precision\": 0, \"recall\": 0, \"f1\": 0},\n",
    "        }\n",
    "    \n",
    "    use_stemmer = True\n",
    "    indic_langs = {\"as\",\"bn\",\"gu\",\"hi\",\"kn\",\"ml\",\"mr\",\"or\",\"pa\",\"ta\",\"te\",\"doi\"}\n",
    "    if lang in indic_langs:\n",
    "        use_stemmer = False\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer = use_stemmer)\n",
    "    metrics = {\n",
    "        \"rouge1\": {\"precision\": [], \"recall\": [], \"f1\": []},\n",
    "        \"rouge2\": {\"precision\": [], \"recall\": [], \"f1\": []},\n",
    "        \"rougeL\": {\"precision\": [], \"recall\": [], \"f1\": []},\n",
    "    }\n",
    "\n",
    "    # loop on summary_knvs\n",
    "    for pred_knv, ref_knv in zip(preds_knv_list, refs_knv_list):\n",
    "        pred_filled = fill_with_template(pred_knv, ref_knv)\n",
    "        pred_values = flatten_knv(pred_filled)\n",
    "        ref_values  = flatten_knv(ref_knv)\n",
    "\n",
    "        # max_len = max(len(pred_values), len(ref_values))\n",
    "        # pred_values += [\"\"] * (max_len - len(pred_values))\n",
    "        # ref_values  += [\"\"] * (max_len - len(ref_values))\n",
    "        \n",
    "        pred_values = [normalize(str(p)) if p is not None else \"None\" for p in pred_values]\n",
    "        ref_values = [normalize(str(r)) if r is not None else \"None\" for r in ref_values]\n",
    "\n",
    "        assert len(pred_values) == len(ref_values), \"Preds and refs must be same length.\"\n",
    "        for p, r in zip(pred_values, ref_values):\n",
    "\n",
    "            if not p and not r:\n",
    "                for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "                    metrics[metric][\"precision\"].append(1.0)\n",
    "                    metrics[metric][\"recall\"].append(1.0)\n",
    "                    metrics[metric][\"f1\"].append(1.0)\n",
    "                continue\n",
    "\n",
    "            if not p or not r:\n",
    "                for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "                    metrics[metric][\"precision\"].append(0.0)\n",
    "                    metrics[metric][\"recall\"].append(0.0)\n",
    "                    metrics[metric][\"f1\"].append(0.0)\n",
    "                continue\n",
    "        \n",
    "            score = scorer.score(r, p)\n",
    "            for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "                metrics[metric][\"precision\"].append(score[metric].precision)\n",
    "                metrics[metric][\"recall\"].append(score[metric].recall)\n",
    "                metrics[metric][\"f1\"].append(score[metric].fmeasure)\n",
    "\n",
    "    avg_metrics = {\n",
    "        metric: {\n",
    "            \"precision\": sum(values[\"precision\"]) / len(values[\"precision\"]) if values['precision'] else 0,\n",
    "            \"recall\": sum(values[\"recall\"]) / len(values[\"recall\"])if values['recall'] else 0,\n",
    "            \"f1\": sum(values[\"f1\"]) / len(values[\"f1\"]) if values['f1'] else 0,\n",
    "        }\n",
    "            for metric, values in metrics.items()\n",
    "    } \n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_knv_cometscore(preds_knv_list, refs_knv_list):\n",
    "    assert len(preds_knv_list) == len(refs_knv_list), \"preds knv and refs knv should be same len\"\n",
    "    if all(p in[\"\", None] or p == {}  for p in preds_knv_list):\n",
    "        return 0.0\n",
    "    hypothesis = []\n",
    "    reference = []\n",
    "    for pred_knv, ref_knv in zip(preds_knv_list, refs_knv_list):\n",
    "        pred_filled = fill_with_template(pred_knv, ref_knv)\n",
    "        pred_values = flatten_knv(pred_filled)\n",
    "        ref_values  = flatten_knv(ref_knv)\n",
    "\n",
    "        pred_values = [normalize(str(p)) if p is not None else \"None\" for p in pred_values]\n",
    "        ref_values = [normalize(str(r)) if r is not None else \"None\" for r in ref_values]\n",
    "\n",
    "        assert len(pred_values) == len(ref_values), \"Preds and refs must be same length.\"\n",
    "        for pred, ref in zip(pred_values, ref_values):\n",
    "            hypothesis.append(pred)\n",
    "            reference.append(ref)\n",
    "\n",
    "    source = [\"\"]*len(hypothesis)\n",
    "\n",
    "    print(source)\n",
    "    print(hypothesis)\n",
    "    print(reference)\n",
    "\n",
    "    results = comet_metric.compute(\n",
    "        predictions=hypothesis,\n",
    "        references=reference,\n",
    "        sources = source,\n",
    "    )\n",
    "    return results['mean_score']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cometscore(preds_list, refs_list):\n",
    "    assert len(preds_list) == len(refs_list), \"Preds and refs should be of same size\"\n",
    "    if all(p == \"\" or p is None for p in preds_list):\n",
    "        return 0.0\n",
    "        \n",
    "    source = [\"\"]*len(preds_list)\n",
    "    hypothesis = [normalize(str(p)) if p is not None else \"None\" for p in preds_list]\n",
    "    reference = [normalize(str(r)) if r is not None else \"None\" for r in refs_list]\n",
    "\n",
    "    results = comet_metric.compute(\n",
    "        predictions=hypothesis,\n",
    "        references=reference,\n",
    "        sources = source,\n",
    "    )\n",
    "    return results['mean_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the file starting with the dialogue ID\n",
    "def find_matching_file(folder, prefix, suffix=None):\n",
    "    for f in os.listdir(folder):\n",
    "        if f.startswith(prefix) and (suffix is None or f.endswith(suffix)):\n",
    "            return os.path.join(folder, f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46661ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46661ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in os.listdir(GT_DIR):\n",
    "\n",
    "\n",
    "    ## Ground Summary text for Dogri is missing, skipping the lang for now\n",
    "    if lang == \"Dogri\":\n",
    "        print(f\"Skipping {lang}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nEvaluating {PRED_DIR} {lang}\")\n",
    "\n",
    "    pred_lang_dir = os.path.join(PRED_DIR, lang)\n",
    "    gt_lang_dir = os.path.join(GT_DIR, lang)\n",
    "\n",
    "    dialogue_dir = os.path.join(gt_lang_dir,'Dialogues')\n",
    "\n",
    "    qna_pred_dir = os.path.join(pred_lang_dir, \"QnA\")\n",
    "    qna_gt_dir = os.path.join(gt_lang_dir, \"QnA\")\n",
    "\n",
    "    summary_text_pred_dir = os.path.join(pred_lang_dir, \"Summary_Text\")\n",
    "    summary_text_gt_dir = os.path.join(gt_lang_dir, \"Summary_Text\")\n",
    "\n",
    "    summary_knv_pred_dir = os.path.join(pred_lang_dir, \"Summary_KnV\")\n",
    "    summary_knv_gt_dir = os.path.join(gt_lang_dir, \"Summary_KnV\")\n",
    "\n",
    "    dialogue_files = [f for f in os.listdir(qna_gt_dir) if f.endswith(\"_questions.json\")]\n",
    "    \n",
    "    dialogue_files.sort()\n",
    "\n",
    "    # Collect all data for batch evaluation\n",
    "    all_qna_preds, all_qna_refs = [], []\n",
    "    all_text_preds, all_text_refs = [], []\n",
    "    all_knv_preds, all_knv_refs = [], []\n",
    "    all_dialogues = []\n",
    "    batch_qna_preds = []\n",
    "    batch_qna_refs = []\n",
    "\n",
    "    for qna_file in tqdm(dialogue_files, desc=f\"{lang}\"):\n",
    "        dialogue_id = qna_file.replace(\"_questions.json\", \"\")\n",
    "        dialogue_file = read_jsonl(os.path.join(dialogue_dir,f\"{dialogue_id}.jsonl\"))\n",
    "\n",
    "        \"\"\"\n",
    "        QnA\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pred_qna = read_json(os.path.join(qna_pred_dir, f\"{TEAM_NAME}_{TASK_NAME}_{TASK_TYPE}_{dialogue_id}_QnA.json\"))[\"questions\"]\n",
    "        except Exception:\n",
    "            pred_qna = None\n",
    "            print(f\"File not Found: {os.path.join(qna_pred_dir, f\"{TEAM_NAME}_{TASK_NAME}_{TASK_TYPE}_{dialogue_id}_QnA.json\")}\")\n",
    "        gt_qna = read_json(find_matching_file(qna_gt_dir, f\"{dialogue_id}_questions.json\"))[\"questions\"]\n",
    "     \n",
    "\n",
    "        \"\"\"\n",
    "        Summary Text\n",
    "        \"\"\"\n",
    "        pred_text_path = os.path.join(summary_text_pred_dir, f\"{TEAM_NAME}_{TASK_NAME}_{TASK_TYPE}_{dialogue_id}_SummaryText.txt\")\n",
    "        try:\n",
    "            pred_text = read_text(pred_text_path)\n",
    "        except Exception:\n",
    "            pred_text = \"\"\n",
    "            print(f\"Flile not found: {pred_text_path}\")\n",
    "        gt_text_path = find_matching_file(summary_text_gt_dir,dialogue_id)\n",
    "        gt_text = read_text(gt_text_path)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        KnV pairs\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pred_knv = read_json(os.path.join(summary_knv_pred_dir, f\"{TEAM_NAME}_{TASK_NAME}_{TASK_TYPE}_{dialogue_id}_SummaryKnV.json\"))\n",
    "        except Exception:\n",
    "            pred_knv = {}\n",
    "            print(f\"File not found: {os.path.join(summary_knv_pred_dir, f\"{TEAM_NAME}_{TASK_NAME}_{TASK_TYPE}_{dialogue_id}_SummaryKnV.json\")}\")\n",
    "        gt_knv = read_json(find_matching_file(summary_knv_gt_dir,dialogue_id))\n",
    "\n",
    "\n",
    "        # Handle case where pred_qna is None\n",
    "        if pred_qna is None:\n",
    "            pred_qna = [{} for _ in range(len(gt_qna))]\n",
    "\n",
    "        # All Answers in a lang added to the list\n",
    "        assert len(pred_qna) == len(gt_qna)\n",
    "        for p, g in zip(pred_qna, gt_qna):\n",
    "            all_qna_preds.append(p.get(\"answer\", \"\").strip())\n",
    "            all_qna_refs.append(g.get(\"answer\", \"\").strip())\n",
    "\n",
    "        # Batched Answers per dialogue\n",
    "        batch_pred =[] \n",
    "        batch_ref = []\n",
    "        for p, g in zip(pred_qna, gt_qna):\n",
    "            batch_pred.append(p.get(\"answer\", \"\").strip())\n",
    "            batch_ref.append(g.get(\"answer\", \"\").strip())\n",
    "\n",
    "        batch_qna_preds.append(batch_pred)\n",
    "        batch_qna_refs.append(batch_ref)\n",
    "\n",
    "\n",
    "        # Text summaries\n",
    "        all_text_preds.append(pred_text)\n",
    "        all_text_refs.append(gt_text)\n",
    "\n",
    "        #KnV pairs\n",
    "        all_knv_preds.append(pred_knv)\n",
    "        all_knv_refs.append(gt_knv)\n",
    "\n",
    "        all_dialogues.append(dialogue_file)\n",
    "\n",
    "\n",
    "    print(f\"QnA Eval for {lang}\")\n",
    "    qna_scores = {\n",
    "        \"f1\": compute_f1_score(all_qna_preds,all_qna_refs),\n",
    "        \"exact_match\":compute_exact_match(all_qna_preds,all_qna_refs),\n",
    "        \"rouge\":compute_rouge(all_qna_preds,all_qna_refs,lang=get_lang_code(lang)),\n",
    "        \"bertscore\":compute_bertscore(all_qna_preds,all_qna_refs, lang=get_lang_code(lang)),\n",
    "        \"cometscore\":compute_cometscore(all_qna_preds,all_qna_refs)\n",
    "        }\n",
    "\n",
    "    print(f\"Summary Eval for {lang}\")\n",
    "\n",
    "    text_scores = {\n",
    "        \"f1\":compute_f1_score(all_text_preds,all_text_refs),\n",
    "        'exact_match':compute_exact_match(all_text_preds,all_text_refs),\n",
    "        \"rouge\": compute_rouge(all_text_preds, all_text_refs,lang = \"en\"),\n",
    "        \"bertscore\": compute_bertscore(all_text_preds, all_text_refs, lang = \"en\"),\n",
    "        \"cometscore\":compute_cometscore(all_text_preds,all_text_refs)\n",
    "        }\n",
    "    \n",
    "\n",
    "    print(f\"KnV Eval for {lang}\")\n",
    "    knv_scores = {\n",
    "        \"f1\":compute_knv_f1_score(all_knv_preds,all_knv_refs),\n",
    "        'exact_match':compute_knv_exact_match(all_knv_preds,all_knv_refs),\n",
    "        \"rouge\": compute_knv_rouge(all_knv_preds, all_knv_refs),\n",
    "        \"bertscore\": compute_knv_bertscore(all_knv_preds, all_knv_refs, lang = \"en\"),\n",
    "        \"cometscore\":compute_knv_cometscore(all_knv_preds,all_knv_refs)\n",
    "        }\n",
    "\n",
    "    results[lang] = {\n",
    "        'qna_scores':qna_scores,\n",
    "        'summary_text_scores':text_scores,\n",
    "        'summary_knv_scores':knv_scores,\n",
    "    }\n",
    "    print(len(all_qna_preds), len(all_text_preds), len(all_knv_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c67455",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join(RESULTS_DIR)\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "out_file_path = os.path.join(out_dir,OUT_FILE)\n",
    "\n",
    "with open(out_file_path,'w') as f:\n",
    "    json.dump(results,f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9911a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpai4health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
